{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-10T04:33:47.884945Z\",\"iopub.execute_input\":\"2023-05-10T04:33:47.885675Z\",\"iopub.status.idle\":\"2023-05-10T04:33:55.778180Z\",\"shell.execute_reply.started\":\"2023-05-10T04:33:47.885633Z\",\"shell.execute_reply\":\"2023-05-10T04:33:55.777057Z\"}}\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Flatten, BatchNormalization\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-10T04:56:52.499425Z\",\"iopub.execute_input\":\"2023-05-10T04:56:52.499834Z\",\"iopub.status.idle\":\"2023-05-10T04:56:52.506646Z\",\"shell.execute_reply.started\":\"2023-05-10T04:56:52.499803Z\",\"shell.execute_reply\":\"2023-05-10T04:56:52.505364Z\"}}\nprint(dataset.columns)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-10T05:02:53.309882Z\",\"iopub.execute_input\":\"2023-05-10T05:02:53.310262Z\",\"iopub.status.idle\":\"2023-05-10T05:02:53.469477Z\",\"shell.execute_reply.started\":\"2023-05-10T05:02:53.310234Z\",\"shell.execute_reply\":\"2023-05-10T05:02:53.468638Z\"}}\ndata_url = '/kaggle/input/world-income-inequality-database'\ndataset = pd.read_csv('/kaggle/input/world-income-inequality-database/WIID_06MAY2020.csv')\n\n# Preprocess data\ndataset = dataset.dropna(subset=['gini_reported', 'gdp_ppp_pc_usd2011'])\ndata = dataset[['gini_reported', 'gdp_ppp_pc_usd2011']].values\ndata = (data - data.min(axis=0)) / (data.max(axis=0) - data.min(axis=0))  # Normalize\n\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-10T05:02:56.819220Z\",\"iopub.execute_input\":\"2023-05-10T05:02:56.819867Z\",\"iopub.status.idle\":\"2023-05-10T05:02:56.827469Z\",\"shell.execute_reply.started\":\"2023-05-10T05:02:56.819831Z\",\"shell.execute_reply\":\"2023-05-10T05:02:56.826357Z\"}}\ndef build_generator(latent_dim):\n    model = Sequential()\n    model.add(Dense(128, input_dim=latent_dim))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(2, activation='tanh'))\n    return model\n\ndef build_discriminator():\n    model = Sequential()\n    model.add(Dense(128, input_dim=2))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(1, activation='sigmoid'))\n    return model\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-10T05:12:26.314768Z\",\"iopub.execute_input\":\"2023-05-10T05:12:26.315563Z\",\"iopub.status.idle\":\"2023-05-10T05:12:46.333438Z\",\"shell.execute_reply.started\":\"2023-05-10T05:12:26.315523Z\",\"shell.execute_reply\":\"2023-05-10T05:12:46.332515Z\"}}\n# Compile and train GAN\n\n# Hyperparameters\nlatent_dim = 100\nbatch_size = 64\nepochs = 200\n\n# Build and compile discriminator\ndiscriminator = build_discriminator()\ndiscriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n\n# Build and compile combined model\ngenerator = build_generator(latent_dim)\nz = Input(shape=(latent_dim,))\nimg = generator(z)\ndiscriminator.trainable = False\nvalidity = discriminator(img)\ncombined = Model(z, validity)\ncombined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Train the GAN\nreal_labels = np.ones((batch_size, 1))\nfake_labels = np.zeros((batch_size, 1))\n\nfor epoch in range(epochs):\n    # Train the discriminator\n    idx = np.random.randint(0, data.shape[0], batch_size)\n    real_data = data[idx]\n\n    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n    fake_data = generator.predict(noise)\n\n    real_loss = discriminator.train_on_batch(real_data, real_labels)\n    fake_loss = discriminator.train_on_batch(fake_data, fake_labels)\n    d_loss = 0.5 * np.add(real_loss, fake_loss)\n\n    # Train the generator\n    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n    g_loss = combined.train_on_batch(noise, real_labels)\n\n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch}/{epochs} [D loss: {d_loss[0]:.4f}, acc.: {100 * d_loss[1]:.2f}%] [G loss: {g_loss:.4f}]\")\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-10T05:18:50.147167Z\",\"iopub.execute_input\":\"2023-05-10T05:18:50.147557Z\",\"iopub.status.idle\":\"2023-05-10T05:18:50.949206Z\",\"shell.execute_reply.started\":\"2023-05-10T05:18:50.147526Z\",\"shell.execute_reply\":\"2023-05-10T05:18:50.948232Z\"}}\n# Generate synthetic data\nnum_samples = 1000\nnoise = np.random.normal(0, 1, (num_samples, latent_dim))\nsynthetic_data = generator.predict(noise)\n\n# Rescale synthetic data to the range [0, 1]\nsynthetic_data = (synthetic_data + 1) / 2\n\n# Visualize synthetic and real data\nplt.scatter(data[:, 0], data[:, 1], label='Real Data', alpha=0.5)\nplt.scatter(synthetic_data[:, 0], synthetic_data[:, 1], label='Synthetic Data', alpha=0.5)\nplt.xlabel('Gini Index')\nplt.ylabel('Income per Capita')\nplt.legend()\nplt.show()\n\n# Unit test\ndef test_synthetic_data(synthetic_data, num_samples, data_shape):\n    assert synthetic_data.shape == (num_samples, data_shape[1]), f\"Expected shape {(num_samples, data_shape[1])}, but got {synthetic_data.shape}\"\n    assert np.all((synthetic_data >= 0) & (synthetic_data <= 1)), \"Synthetic data should be within the range [0, 1]\"\n\n# Run unit test\ntest_synthetic_data(synthetic_data, num_samples, data.shape)","metadata":{"_uuid":"af09a6fa-97e2-40d8-9e60-b5fe4277a364","_cell_guid":"183bcac0-2edb-4712-bbb8-3fdaac87c345","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}