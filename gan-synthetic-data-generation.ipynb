{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Flatten, BatchNormalization\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\n","metadata":{"execution":{"iopub.status.busy":"2023-05-10T04:33:47.884945Z","iopub.execute_input":"2023-05-10T04:33:47.885675Z","iopub.status.idle":"2023-05-10T04:33:55.778180Z","shell.execute_reply.started":"2023-05-10T04:33:47.885633Z","shell.execute_reply":"2023-05-10T04:33:55.777057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dataset.columns)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T04:56:52.499425Z","iopub.execute_input":"2023-05-10T04:56:52.499834Z","iopub.status.idle":"2023-05-10T04:56:52.506646Z","shell.execute_reply.started":"2023-05-10T04:56:52.499803Z","shell.execute_reply":"2023-05-10T04:56:52.505364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_url = '/kaggle/input/world-income-inequality-database'\ndataset = pd.read_csv('/kaggle/input/world-income-inequality-database/WIID_06MAY2020.csv')\n\n# Preprocess data\ndataset = dataset.dropna(subset=['gini_reported', 'gdp_ppp_pc_usd2011'])\ndata = dataset[['gini_reported', 'gdp_ppp_pc_usd2011']].values\ndata = (data - data.min(axis=0)) / (data.max(axis=0) - data.min(axis=0))  # Normalize\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:02:53.309882Z","iopub.execute_input":"2023-05-10T05:02:53.310262Z","iopub.status.idle":"2023-05-10T05:02:53.469477Z","shell.execute_reply.started":"2023-05-10T05:02:53.310234Z","shell.execute_reply":"2023-05-10T05:02:53.468638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_generator(latent_dim):\n    model = Sequential()\n    model.add(Dense(128, input_dim=latent_dim))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(2, activation='tanh'))\n    return model\n\ndef build_discriminator():\n    model = Sequential()\n    model.add(Dense(128, input_dim=2))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(1, activation='sigmoid'))\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:02:56.819220Z","iopub.execute_input":"2023-05-10T05:02:56.819867Z","iopub.status.idle":"2023-05-10T05:02:56.827469Z","shell.execute_reply.started":"2023-05-10T05:02:56.819831Z","shell.execute_reply":"2023-05-10T05:02:56.826357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile and train GAN\n\n# Hyperparameters\nlatent_dim = 100\nbatch_size = 64\nepochs = 200\n\n# Build and compile discriminator\ndiscriminator = build_discriminator()\ndiscriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n\n# Build and compile combined model\ngenerator = build_generator(latent_dim)\nz = Input(shape=(latent_dim,))\nimg = generator(z)\ndiscriminator.trainable = False\nvalidity = discriminator(img)\ncombined = Model(z, validity)\ncombined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Train the GAN\nreal_labels = np.ones((batch_size, 1))\nfake_labels = np.zeros((batch_size, 1))\n\nfor epoch in range(epochs):\n    # Train the discriminator\n    idx = np.random.randint(0, data.shape[0], batch_size)\n    real_data = data[idx]\n\n    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n    fake_data = generator.predict(noise)\n\n    real_loss = discriminator.train_on_batch(real_data, real_labels)\n    fake_loss = discriminator.train_on_batch(fake_data, fake_labels)\n    d_loss = 0.5 * np.add(real_loss, fake_loss)\n\n    # Train the generator\n    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n    g_loss = combined.train_on_batch(noise, real_labels)\n\n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch}/{epochs} [D loss: {d_loss[0]:.4f}, acc.: {100 * d_loss[1]:.2f}%] [G loss: {g_loss:.4f}]\")\n","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:12:26.314768Z","iopub.execute_input":"2023-05-10T05:12:26.315563Z","iopub.status.idle":"2023-05-10T05:12:46.333438Z","shell.execute_reply.started":"2023-05-10T05:12:26.315523Z","shell.execute_reply":"2023-05-10T05:12:46.332515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate synthetic data\nnum_samples = 1000\nnoise = np.random.normal(0, 1, (num_samples, latent_dim))\nsynthetic_data = generator.predict(noise)\n\n# Rescale synthetic data to the range [0, 1]\nsynthetic_data = (synthetic_data + 1) / 2\n\n# Visualize synthetic and real data\nplt.scatter(data[:, 0], data[:, 1], label='Real Data', alpha=0.5)\nplt.scatter(synthetic_data[:, 0], synthetic_data[:, 1], label='Synthetic Data', alpha=0.5)\nplt.xlabel('Gini Index')\nplt.ylabel('Income per Capita')\nplt.legend()\nplt.show()\n\n# Unit test\ndef test_synthetic_data(synthetic_data, num_samples, data_shape):\n    assert synthetic_data.shape == (num_samples, data_shape[1]), f\"Expected shape {(num_samples, data_shape[1])}, but got {synthetic_data.shape}\"\n    assert np.all((synthetic_data >= 0) & (synthetic_data <= 1)), \"Synthetic data should be within the range [0, 1]\"\n\n# Run unit test\ntest_synthetic_data(synthetic_data, num_samples, data.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T05:18:50.147167Z","iopub.execute_input":"2023-05-10T05:18:50.147557Z","iopub.status.idle":"2023-05-10T05:18:50.949206Z","shell.execute_reply.started":"2023-05-10T05:18:50.147526Z","shell.execute_reply":"2023-05-10T05:18:50.948232Z"},"trusted":true},"execution_count":null,"outputs":[]}]}